{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-1.279952713488994 -1.279952713488994 4.55872326420284 7.55872326420284\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,4.998817837224852)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.15117446528405679\" opacity=\"0.6\" d=\"M 0.09703597891847811,1.511249541291573 L 1.0298574998546681,5.242535625036333 L 1.0583016495538902,5.33645834328647 L 1.095814869152591,5.427140783763917 L 1.1420358858392228,5.5137096249948145 L 1.196519565956116,5.595331161714766 L 1.258741201879936,5.671219333906286 L 1.3281015652429293,5.740643296986477 L 1.4039326778296797,5.802934460239878 L 1.4855042445724735,5.8574929257125445 L 1.5720306866920237,5.903793265557284 L 1.6626787072505538,5.941389582191052 L 1.756575316256837,5.969919802532426 L 1.852816238037035,5.989109164963317 L 1.950474619903765,5.99877286543354 L 2.0486099582541484,5.998817837224852 L 2.1462771561336376,5.989243647234318 L 2.242535625036333,5.970142500145332 L 2.3364583432864694,5.94169835044611 L 2.4271407837639174,5.904185130847409 L 2.5137096249948145,5.857964114160777 L 2.5953311617147654,5.803480434043884 L 2.671219333906286,5.7412587981200645 L 2.7406432969864767,5.671898434757071 L 2.802934460239878,5.59606732217032 L 2.857492925712544,5.514495755427527 L 2.903793265557284,5.427969313307976 L 2.9413895821910514,5.337321292749446 L 2.9699198025324263,5.2434246837431635 L 2.9891091649633164,5.147183761962965 L 2.9987728654335397,5.049525380096235 L 2.9988178372248515,4.951390041745852 L 2.989243647234318,4.853722843866362 L 2.970142500145332,4.757464374963667 L 1.9701425001453319,0.757464374963667 L 1.9443943369650265,0.6711849512136219 L 1.910989867967712,0.5875712662044863 L 1.870199906753479,0.5073011850164071 L 1.8223551418971056,0.4310254657760233 L 1.7678434560204437,0.35936248389078773 L 1.7071067811865475,0.29289321881345254 L 0.7071067811865475,-0.7071067811865475 L 0.6343932841636455,-0.773010453362737 L 0.5555702330196023,-0.8314696123025452 L 0.4713967368259978,-0.8819212643483549 L 0.38268343236508984,-0.9238795325112867 L 0.29028467725446233,-0.9569403357322089 L 0.19509032201612833,-0.9807852804032304 L 0.09801714032956077,-0.9951847266721968 L 6.123233995736766e-17,-1.0 L -0.09801714032956065,-0.9951847266721969 L -0.1950903220161282,-0.9807852804032304 L -0.29028467725446216,-0.9569403357322089 L -0.3826834323650897,-0.9238795325112867 L -0.4713967368259977,-0.881921264348355 L -0.555570233019602,-0.8314696123025455 L -0.6343932841636454,-0.7730104533627371 L -0.7071067811865475,-0.7071067811865476 L -0.773010453362737,-0.6343932841636455 L -0.8314696123025453,-0.5555702330196022 L -0.8819212643483549,-0.47139673682599786 L -0.9238795325112867,-0.3826834323650899 L -0.9569403357322088,-0.2902846772544624 L -0.9807852804032304,-0.1950903220161286 L -0.9951847266721968,-0.09801714032956083 L -1.0,-1.2246467991473532e-16 L -0.9951847266721969,0.09801714032956059 L -0.9807852804032304,0.19509032201612836 L -0.9569403357322089,0.2902846772544621 L -0.9238795325112868,0.38268343236508967 L -0.881921264348355,0.47139673682599764 L -0.8314696123025455,0.555570233019602 L -0.7730104533627371,0.6343932841636453 L -0.7071067811865475,0.7071067811865475 L 0.09703597891847811,1.511249541291573 z\" /></g></svg>",
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x7fec614356c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shapely.geometry import Point, LineString\n",
    "LineString([(0, 0), (1, 1),(2,5)]).buffer(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((2.000 1.000, 2.000 2.000, 1.000 2.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((200.000 100.000, 200.000 300.000, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   h                                           geometry\n",
       "0  1  POLYGON ((2.000 1.000, 2.000 2.000, 1.000 2.00...\n",
       "1  2  POLYGON ((200.000 100.000, 200.000 300.000, 0...."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import box\n",
    "d=GeoDataFrame([{\"h\": \"1\", \"geometry\": box(1,1,2,2)},{\"h\": \"2\", \"geometry\": box(0,100,200,300)}])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.sindex.query(LineString([(0, 0), (1, 1),(100,100)]).buffer(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12970025.341848467, 4854502.231468809)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyproj import Transformer\n",
    "transformer = Transformer.from_crs(4326, 3857,always_xy=True)\n",
    "lng,lat=116.51172,39.92123\n",
    "transformer.transform(lng,lat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2219\n",
      "1118\n",
      "2599\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from shapely import wkt\n",
    "import sys\n",
    "h = LineString([(0, 0), (1, 1)]).buffer(1).wkb_hex\n",
    "b=LineString([(0, 0), (1, 1)]).buffer(1).wkb\n",
    "s = LineString([(0, 0), (1, 1)]).buffer(1).wkt\n",
    "print(sys.getsizeof(h))\n",
    "print(sys.getsizeof(b))\n",
    "print(sys.getsizeof(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 4]\n",
      "1 [453, 5]\n",
      "2 [6, 265]\n",
      "3 [234, 298]\n",
      "4 [45, 23]\n",
      "5 [888, 24]\n",
      "a\n",
      "1 2 3 4\n"
     ]
    }
   ],
   "source": [
    "from shapely import wkb\n",
    "wkb.loads(b)\n",
    "# wkt.loads(s)\n",
    "from split import chop\n",
    "\n",
    "for idx,v in enumerate(chop(2,{1,23,4,5,6,45,888,24,265,234,453,5,298})):\n",
    "    print(idx,v)\n",
    "print(Exception(\"a\"))\n",
    "LineString([(0, 0), (1, 1)]).buffer(1).wkt\n",
    "print(*{1,2,3,4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MovingPandas 0.9.rc3\n",
      "\n",
      "SYSTEM INFO\n",
      "-----------\n",
      "python     : 3.10.2 | packaged by conda-forge | (main, Mar  8 2022, 15:53:57) [GCC 9.4.0]\n",
      "executable : /home/liontao/miniconda3/envs/beam/bin/python\n",
      "machine    : Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\n",
      "\n",
      "GEOS, GDAL, PROJ INFO\n",
      "---------------------\n",
      "GEOS       : 3.10.2\n",
      "GEOS lib   : /home/liontao/miniconda3/envs/beam/lib/libgeos_c.so\n",
      "GDAL       : 3.4.1\n",
      "GDAL data dir: /home/liontao/miniconda3/envs/beam/share/gdal\n",
      "PROJ       : 8.2.1\n",
      "PROJ data dir: /home/liontao/miniconda3/envs/beam/share/proj\n",
      "\n",
      "PYTHON DEPENDENCIES\n",
      "-------------------\n",
      "geopandas  : 0.10.2\n",
      "pandas     : 1.4.1\n",
      "fiona      : 1.8.21\n",
      "numpy      : 1.22.3\n",
      "shapely    : 1.8.0\n",
      "rtree      : 0.9.7\n",
      "pyproj     : 3.3.0\n",
      "matplotlib : 3.5.1\n",
      "mapclassify: 2.4.3\n",
      "geopy      : 2.2.0\n",
      "holoviews  : 1.14.8\n",
      "hvplot     : 0.7.3\n",
      "geoviews   : 1.9.5\n",
      "stonesoup  : 0.1b8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopandas import GeoDataFrame, read_file\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import movingpandas as mpd\n",
    "mpd.show_versions()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('data/all.txt', delimiter=',')\n",
    "raw_df.columns = ['trajectory_id','t','X','Y']\n",
    "traj_collection = mpd.TrajectoryCollection(raw_df, 'trajectory_id', t='t', x='X', y='Y')\n",
    "splitted = mpd.TemporalSplitter(traj_collection).split(mode=\"day\",min_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=[t for t in filter(lambda x:x.size()<200 and x.size()>50 and x.get_length()<200000, splitted)]\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trajectory_id</th>\n",
       "      <th>geometry</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-02-03 00:00:32</th>\n",
       "      <td>1</td>\n",
       "      <td>POINT (116.69171 39.85184)</td>\n",
       "      <td>116.69171</td>\n",
       "      <td>39.85184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-03 00:10:32</th>\n",
       "      <td>1</td>\n",
       "      <td>POINT (116.69170 39.85184)</td>\n",
       "      <td>116.69170</td>\n",
       "      <td>39.85184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-03 00:20:32</th>\n",
       "      <td>1</td>\n",
       "      <td>POINT (116.69170 39.85184)</td>\n",
       "      <td>116.69170</td>\n",
       "      <td>39.85184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-03 00:30:32</th>\n",
       "      <td>1</td>\n",
       "      <td>POINT (116.69168 39.85146)</td>\n",
       "      <td>116.69168</td>\n",
       "      <td>39.85146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-03 00:40:32</th>\n",
       "      <td>1</td>\n",
       "      <td>POINT (116.69172 39.85165)</td>\n",
       "      <td>116.69172</td>\n",
       "      <td>39.85165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-03 23:15:12</th>\n",
       "      <td>1</td>\n",
       "      <td>POINT (116.69160 39.85181)</td>\n",
       "      <td>116.69160</td>\n",
       "      <td>39.85181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-03 23:25:12</th>\n",
       "      <td>1</td>\n",
       "      <td>POINT (116.69163 39.85195)</td>\n",
       "      <td>116.69163</td>\n",
       "      <td>39.85195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-03 23:35:13</th>\n",
       "      <td>1</td>\n",
       "      <td>POINT (116.69159 39.85173)</td>\n",
       "      <td>116.69159</td>\n",
       "      <td>39.85173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-03 23:45:12</th>\n",
       "      <td>1</td>\n",
       "      <td>POINT (116.69158 39.85175)</td>\n",
       "      <td>116.69158</td>\n",
       "      <td>39.85175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-03 23:55:13</th>\n",
       "      <td>1</td>\n",
       "      <td>POINT (116.69160 39.85177)</td>\n",
       "      <td>116.69160</td>\n",
       "      <td>39.85177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     trajectory_id                    geometry          X  \\\n",
       "t                                                                           \n",
       "2008-02-03 00:00:32              1  POINT (116.69171 39.85184)  116.69171   \n",
       "2008-02-03 00:10:32              1  POINT (116.69170 39.85184)  116.69170   \n",
       "2008-02-03 00:20:32              1  POINT (116.69170 39.85184)  116.69170   \n",
       "2008-02-03 00:30:32              1  POINT (116.69168 39.85146)  116.69168   \n",
       "2008-02-03 00:40:32              1  POINT (116.69172 39.85165)  116.69172   \n",
       "...                            ...                         ...        ...   \n",
       "2008-02-03 23:15:12              1  POINT (116.69160 39.85181)  116.69160   \n",
       "2008-02-03 23:25:12              1  POINT (116.69163 39.85195)  116.69163   \n",
       "2008-02-03 23:35:13              1  POINT (116.69159 39.85173)  116.69159   \n",
       "2008-02-03 23:45:12              1  POINT (116.69158 39.85175)  116.69158   \n",
       "2008-02-03 23:55:13              1  POINT (116.69160 39.85177)  116.69160   \n",
       "\n",
       "                            Y  \n",
       "t                              \n",
       "2008-02-03 00:00:32  39.85184  \n",
       "2008-02-03 00:10:32  39.85184  \n",
       "2008-02-03 00:20:32  39.85184  \n",
       "2008-02-03 00:30:32  39.85146  \n",
       "2008-02-03 00:40:32  39.85165  \n",
       "...                       ...  \n",
       "2008-02-03 23:15:12  39.85181  \n",
       "2008-02-03 23:25:12  39.85195  \n",
       "2008-02-03 23:35:13  39.85173  \n",
       "2008-02-03 23:45:12  39.85175  \n",
       "2008-02-03 23:55:13  39.85177  \n",
       "\n",
       "[96 rows x 4 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = res[0].to_point_gdf()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index()\n",
    "df['X']=df[\"geometry\"].x\n",
    "df['Y']=df[\"geometry\"].y\n",
    "df.columns\n",
    "df.to_csv(columns=[\"trajectory_id\",\"t\",\"X\",\"Y\"],header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:00<00:00, 451.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "for i,t in enumerate(tqdm(res)):\n",
    "    df = t.to_point_gdf().reset_index()\n",
    "    df['X']=df[\"geometry\"].x\n",
    "    df['Y']=df[\"geometry\"].y\n",
    "    df[\"trajectory_id\"]=i+1\n",
    "    with open(f\"data/filtered/{i+1}.txt\",'w') as f:\n",
    "        df.to_csv(f,columns=[\"trajectory_id\",\"t\",\"X\",\"Y\"],header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17712"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=0\n",
    "for i in range(1,151):\n",
    "    with open(f\"data/filtered/{i}.txt\") as f:\n",
    "        c+=len(f.readlines())\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "bucket = []\n",
    "for i in range(200,1001,100):\n",
    "    with open(f\"/mnt/g/Tencent/results_backup/{i}_0.2_2000.json\") as f:\n",
    "        bucket.append(json.load(f))\n",
    "df=pd.DataFrame(bucket)\n",
    "df = df.rename(columns={\"t1\":\"insertion time\",\"t2\":\"query time\"})\n",
    "df.to_excel(\"/mnt/g/Tencent/results_backup/m-varient.xlsx\",columns=[\"insertion time\",\"query time\",\"tps\",\"latency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n",
      "3006\n",
      "3007\n",
      "3008\n",
      "3009\n",
      "3010\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "((p=3000+$i))\n",
    "echo $p\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import os\n",
    "trajectories=list(range(1,101))\n",
    "settings=[]\n",
    "template = {\n",
    "        \"result_file_name\": \"test\",\n",
    "        \"trajectories\": trajectories,\n",
    "        \"INIT_RESOLUTION\": 1,\n",
    "        \"MAX_BUFFER_SIZE\": 500,\n",
    "        \"TREE_INSERTION_THRESHOLD\": 0.2,\n",
    "        \"SPLIT_THRESHOLD\": 2000\n",
    "}\n",
    "settings = []\n",
    "for m in [0, 2, 20, 200, 500, 700]:\n",
    "    temp = deepcopy(template)\n",
    "    temp[\"MAX_BUFFER_SIZE\"] = m\n",
    "    temp[\n",
    "        \"result_file_name\"] = f'{temp[\"MAX_BUFFER_SIZE\"]}_{temp[\"TREE_INSERTION_THRESHOLD\"]}_{temp[\"SPLIT_THRESHOLD\"]}_{temp[\"INIT_RESOLUTION\"]}'\n",
    "    settings.append(temp)\n",
    "for i in [0,5,10]:\n",
    "    temp = deepcopy(template)\n",
    "    temp[\"INIT_RESOLUTION\"] = i\n",
    "    temp[\n",
    "        \"result_file_name\"] = f'{temp[\"MAX_BUFFER_SIZE\"]}_{temp[\"TREE_INSERTION_THRESHOLD\"]}_{temp[\"SPLIT_THRESHOLD\"]}_{temp[\"INIT_RESOLUTION\"]}'\n",
    "    settings.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import iglob\n",
    "import json\n",
    "res=dict()\n",
    "query_all=dict()\n",
    "tops_all=dict()\n",
    "for s in settings:\n",
    "    if os.path.exists(f\"tests/results/{s['result_file_name']}.json\"):\n",
    "        query=[]\n",
    "        tops=[]\n",
    "        with open(f\"tests/results/{s['result_file_name']}.json\") as f:\n",
    "            res[s['result_file_name']]=json.load(f)\n",
    "        for fname in iglob(os.path.abspath(f\"tests/results/{s['result_file_name']}*\")):\n",
    "            if \"top\" in fname:\n",
    "                with open(fname) as f:\n",
    "                    tops.append(json.load(f))\n",
    "            elif \"query\" in fname:\n",
    "                with open(fname) as f:\n",
    "                    query.append(json.load(f))\n",
    "        query_all[s['result_file_name']]=query\n",
    "        tops_all[s['result_file_name']]=tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for s in settings:\n",
    "    m,r,k,resolution = s['result_file_name'].split(\"_\")[:5]\n",
    "    # res_df = pd.DataFrame(res[s['result_file_name']])\n",
    "    # res_df[\"M\"]=m\n",
    "    # res_df[\"R\"]=r\n",
    "    # res_df[\"K\"]=k\n",
    "    # res_df[\"init_resolution\"]=resolution\n",
    "    query_df = pd.DataFrame(query_all[s['result_file_name']])\n",
    "    query_df[\"M\"]=m\n",
    "    query_df[\"R\"]=r\n",
    "    query_df[\"K\"]=k\n",
    "    query_df[\"init_resolution\"]=resolution\n",
    "    query_df=query_df.set_index(\"time\")\n",
    "    query_df.to_excel(f\"{s['result_file_name']}_query.xlsx\")\n",
    "    top_df = pd.DataFrame(tops_all[s['result_file_name']])\n",
    "    top_df[\"M\"]=m\n",
    "    top_df[\"R\"]=r\n",
    "    top_df[\"K\"]=k\n",
    "    top_df[\"init_resolution\"]=resolution\n",
    "    top_df=top_df.set_index(\"time\")\n",
    "    top_df.to_excel(f\"{s['result_file_name']}_top.xlsx\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "for m in [0, 2, 20, 200, 500, 700]:\n",
    "    temp = deepcopy(template)\n",
    "    temp[\"MAX_BUFFER_SIZE\"] = m\n",
    "    temp[\"result_file_name\"] = f'{temp[\"MAX_BUFFER_SIZE\"]}_{temp[\"TREE_INSERTION_THRESHOLD\"]}_{temp[\"SPLIT_THRESHOLD\"]}_{temp[\"INIT_RESOLUTION\"]}'\n",
    "    m,r,k,resolution = temp[\"result_file_name\"].split(\"_\")[:5]\n",
    "    with open(f\"tests/results/{temp['result_file_name']}.json\") as f:\n",
    "        data=json.load(f)\n",
    "        data[\"M\"]=m\n",
    "        data[\"R\"]=r\n",
    "        data[\"K\"]=k\n",
    "        data[\"init_resolution\"]=resolution\n",
    "        res.append(data)\n",
    "pd.DataFrame(res).to_excel(\"m.xlsx\")\n",
    "\n",
    "res=[]\n",
    "for i in [0,5,10]:\n",
    "    temp = deepcopy(template)\n",
    "    temp[\"INIT_RESOLUTION\"] = i\n",
    "    temp[\n",
    "        \"result_file_name\"] = f'{temp[\"MAX_BUFFER_SIZE\"]}_{temp[\"TREE_INSERTION_THRESHOLD\"]}_{temp[\"SPLIT_THRESHOLD\"]}_{temp[\"INIT_RESOLUTION\"]}'\n",
    "    m,r,k,resolution = temp[\"result_file_name\"].split(\"_\")[:5]\n",
    "    with open(f\"tests/results/{temp['result_file_name']}.json\") as f:\n",
    "        data=json.load(f)\n",
    "        data[\"M\"]=m\n",
    "        data[\"R\"]=r\n",
    "        data[\"K\"]=k\n",
    "        data[\"init_resolution\"]=resolution\n",
    "        res.append(data)\n",
    "pd.DataFrame(res).to_excel(\"init_resolution.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有关SPLIT_THRESHOLD的插入时间图表\n",
    "# index是分裂阈值：[1000, 1500, 2000, 2500, 3000]\n",
    "# 横坐标是init_resolution=1\n",
    "# 一张图是默认参数，m=500\n",
    "# 另一张图是特殊参数，m=0\n",
    "import json\n",
    "import pandas as  pd\n",
    "settings_default=[]\n",
    "settings_nonbuffer=[]\n",
    "for k in [1000, 1500, 2000, 2500, 3000]:\n",
    "    temp = deepcopy(template)\n",
    "    temp[\"SPLIT_THRESHOLD\"] = k\n",
    "    temp[\n",
    "        \"result_file_name\"] = f'{temp[\"MAX_BUFFER_SIZE\"]}_{temp[\"TREE_INSERTION_THRESHOLD\"]}_{temp[\"SPLIT_THRESHOLD\"]}_{temp[\"INIT_RESOLUTION\"]}'\n",
    "    settings_default.append(temp)\n",
    "    temp = deepcopy(temp)\n",
    "    temp[\"MAX_BUFFER_SIZE\"] = 0\n",
    "    temp[\n",
    "        \"result_file_name\"] = f'{temp[\"MAX_BUFFER_SIZE\"]}_{temp[\"TREE_INSERTION_THRESHOLD\"]}_{temp[\"SPLIT_THRESHOLD\"]}_{temp[\"INIT_RESOLUTION\"]}'\n",
    "    settings_nonbuffer.append(temp)\n",
    "\n",
    "bucket = []\n",
    "for fname in settings_default:\n",
    "    m,r,k,resolution = fname[\"result_file_name\"].split(\"_\")[:5]\n",
    "    with open(f\"tests/results/{fname['result_file_name']}.json\") as f:\n",
    "        data=json.load(f)\n",
    "        data[\"M\"]=m\n",
    "        data[\"R\"]=r\n",
    "        data[\"K\"]=k\n",
    "        data[\"init_resolution\"]=resolution\n",
    "        bucket.append(data)\n",
    "pd.DataFrame(bucket).set_index(\"K\").to_excel(\"开启缓冲k变量的插入时间-init=10.xlsx\",columns=[\"insertion_time\"])\n",
    "\n",
    "bucket = []\n",
    "for fname in settings_nonbuffer:\n",
    "    m,r,k,resolution = fname[\"result_file_name\"].split(\"_\")[:5]\n",
    "    with open(f\"tests/results/{fname['result_file_name']}.json\") as f:\n",
    "        data=json.load(f)\n",
    "        data[\"M\"]=m\n",
    "        data[\"R\"]=r\n",
    "        data[\"K\"]=k\n",
    "        data[\"init_resolution\"]=resolution\n",
    "        bucket.append(data)\n",
    "pd.DataFrame(bucket).set_index(\"K\").to_excel(\"关闭缓冲k变量的插入时间-init=10.xlsx\",columns=[\"insertion_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20848/1493171724.py:22: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/1493171724.py:23: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n",
      "/tmp/ipykernel_20848/1493171724.py:22: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/1493171724.py:23: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n",
      "/tmp/ipykernel_20848/1493171724.py:22: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/1493171724.py:23: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n",
      "/tmp/ipykernel_20848/1493171724.py:22: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/1493171724.py:23: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n",
      "/tmp/ipykernel_20848/1493171724.py:22: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/1493171724.py:23: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n"
     ]
    }
   ],
   "source": [
    "# 有关SPLIT_THRESHOLD的性能监控\n",
    "# 有buffer部分\n",
    "from glob import iglob\n",
    "top_res = []\n",
    "query_res=[]\n",
    "for s in settings_default:\n",
    "    tops=[]\n",
    "    query=[]\n",
    "    for fname in iglob(os.path.abspath(f\"tests/results/{s['result_file_name']}*\")):\n",
    "        if \"top\" in fname:\n",
    "            with open(fname) as f:\n",
    "                tops.append(json.load(f))\n",
    "        elif \"query\" in fname:\n",
    "            with open(fname) as f:\n",
    "                query.append(json.load(f))\n",
    "    m,r,k,resolution = s[\"result_file_name\"].split(\"_\")[:5]\n",
    "    tops = pd.DataFrame(tops)\n",
    "    cpu_avg = tops.mean(axis=0)[\"cpu\"]\n",
    "    mem_avg = tops.mean(axis=0)[\"mem\"]\n",
    "    top_res.append({\"K\":k,\"cpu_avg\":cpu_avg,\"mem_avg\":mem_avg})\n",
    "    query = pd.DataFrame(query)\n",
    "    regions_avg = query.mean(axis=0)[\"regions\"]\n",
    "    tid_avg = query.mean(axis=0)[\"tid\"]\n",
    "    query_res.append({\"K\":k,\"query_latency\":regions_avg+tid_avg})\n",
    "pd.DataFrame(top_res).set_index(\"K\").to_excel(\"开启缓冲-k变量性能监控init=10.xlsx\")\n",
    "pd.DataFrame(query_res).set_index(\"K\").to_excel(\"开启缓冲-k变量查询延迟init=10.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20848/3430710689.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/3430710689.py:22: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n",
      "/tmp/ipykernel_20848/3430710689.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/3430710689.py:22: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n",
      "/tmp/ipykernel_20848/3430710689.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/3430710689.py:22: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n",
      "/tmp/ipykernel_20848/3430710689.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/3430710689.py:22: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n",
      "/tmp/ipykernel_20848/3430710689.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/3430710689.py:22: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n"
     ]
    }
   ],
   "source": [
    "# 有关SPLIT_THRESHOLD的性能监控\n",
    "# 无buffer部分\n",
    "top_res = []\n",
    "query_res=[]\n",
    "for s in settings_nonbuffer:\n",
    "    tops=[]\n",
    "    query=[]\n",
    "    for fname in iglob(os.path.abspath(f\"tests/results/{s['result_file_name']}*\")):\n",
    "        if \"top\" in fname:\n",
    "            with open(fname) as f:\n",
    "                tops.append(json.load(f))\n",
    "        elif \"query\" in fname:\n",
    "            with open(fname) as f:\n",
    "                query.append(json.load(f))\n",
    "    m,r,k,resolution = s[\"result_file_name\"].split(\"_\")[:5]\n",
    "    tops = pd.DataFrame(tops)\n",
    "    cpu_avg = tops.mean(axis=0)[\"cpu\"]\n",
    "    mem_avg = tops.mean(axis=0)[\"mem\"]\n",
    "    top_res.append({\"K\":k,\"cpu_avg\":cpu_avg,\"mem_avg\":mem_avg})\n",
    "    query = pd.DataFrame(query)\n",
    "    regions_avg = query.mean(axis=0)[\"regions\"]\n",
    "    tid_avg = query.mean(axis=0)[\"tid\"]\n",
    "    query_res.append({\"K\":k,\"query_latency\":regions_avg+tid_avg})\n",
    "pd.DataFrame(top_res).set_index(\"K\").to_excel(\"关闭缓冲-k变量性能监控init=10.xlsx\")\n",
    "pd.DataFrame(query_res).set_index(\"K\").to_excel(\"关闭缓冲-k变量查询延迟init=10.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resolution='10'\n",
      "resolution='10'\n",
      "resolution='10'\n",
      "resolution='10'\n",
      "resolution='10'\n"
     ]
    }
   ],
   "source": [
    "# 有关TREE_INSERTION_THRESHOLD的插入时间图表\n",
    "# TREE_INSERTION_THRESHOLD是插入阈值：[1000, 1500, 2000, 2500, 3000]\n",
    "# 横坐标是init_resolution=1\n",
    "# 默认参数，m=500\n",
    "import json\n",
    "import pandas as  pd\n",
    "settings_default=[]\n",
    "for t in [0, 0.1, 0.3, 0.5, 0.9]:\n",
    "    temp = deepcopy(template)\n",
    "    temp[\"TREE_INSERTION_THRESHOLD\"] = t\n",
    "    temp[\n",
    "        \"result_file_name\"] = f'{temp[\"MAX_BUFFER_SIZE\"]}_{temp[\"TREE_INSERTION_THRESHOLD\"]}_{temp[\"SPLIT_THRESHOLD\"]}_{temp[\"INIT_RESOLUTION\"]}'\n",
    "    settings_default.append(temp)\n",
    "    \n",
    "\n",
    "bucket = []\n",
    "for fname in settings_default:\n",
    "    m,r,k,resolution = fname[\"result_file_name\"].split(\"_\")[:5]\n",
    "    print(f\"{resolution=}\")\n",
    "    with open(f\"tests/results/{fname['result_file_name']}.json\") as f:\n",
    "        data=json.load(f)\n",
    "        data[\"M\"]=m\n",
    "        data[\"R\"]=r\n",
    "        data[\"K\"]=k\n",
    "        data[\"init_resolution\"]=resolution\n",
    "        bucket.append(data)\n",
    "pd.DataFrame(bucket).set_index(\"R\").to_excel(\"r变量的插入时间init=10.xlsx\",columns=[\"insertion_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20848/1104950859.py:20: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/1104950859.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n",
      "/tmp/ipykernel_20848/1104950859.py:20: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/1104950859.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n",
      "/tmp/ipykernel_20848/1104950859.py:20: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/1104950859.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n",
      "/tmp/ipykernel_20848/1104950859.py:20: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/1104950859.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n",
      "/tmp/ipykernel_20848/1104950859.py:20: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_20848/1104950859.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n"
     ]
    }
   ],
   "source": [
    "# 有关TREE_INSERTION_THRESHOLD的性能监控\n",
    "top_res = []\n",
    "query_res=[]\n",
    "for s in settings_default:\n",
    "    tops=[]\n",
    "    query=[]\n",
    "    for fname in iglob(os.path.abspath(f\"tests/results/{s['result_file_name']}*\")):\n",
    "        if \"top\" in fname:\n",
    "            with open(fname) as f:\n",
    "                tops.append(json.load(f))\n",
    "        elif \"query\" in fname:\n",
    "            with open(fname) as f:\n",
    "                query.append(json.load(f))\n",
    "    m,r,k,resolution = s[\"result_file_name\"].split(\"_\")[:5]\n",
    "    tops = pd.DataFrame(tops)\n",
    "    cpu_avg = tops.mean(axis=0)[\"cpu\"]\n",
    "    mem_avg = tops.mean(axis=0)[\"mem\"]\n",
    "    top_res.append({\"R\":r,\"cpu_avg\":cpu_avg,\"mem_avg\":mem_avg})\n",
    "    query = pd.DataFrame(query)\n",
    "    regions_avg = query.mean(axis=0)[\"regions\"]\n",
    "    tid_avg = query.mean(axis=0)[\"tid\"]\n",
    "    query_res.append({\"R\":r,\"query_latency\":regions_avg+tid_avg})\n",
    "pd.DataFrame(top_res).set_index(\"R\").to_excel(\"r变量性能监控init=10.xlsx\")\n",
    "pd.DataFrame(query_res).set_index(\"R\").to_excel(\"r变量查询延迟init=10.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insertion time: 2330.354618311998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30279/1005184627.py:33: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_30279/1005184627.py:34: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cpu_avg</th>\n",
       "      <th>mem_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.042857</td>\n",
       "      <td>14.838095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cpu_avg    mem_avg\n",
       "R                     \n",
       "0  6.042857  14.838095"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline 朴素rtree\n",
    "temp = deepcopy(template)\n",
    "temp[\"SPLIT_THRESHOLD\"] = float('inf')\n",
    "temp[\"TREE_INSERTION_THRESHOLD\"] = 0\n",
    "temp[\"MAX_BUFFER_SIZE\"] = 0\n",
    "temp[\"INIT_RESOLUTION\"] = 0\n",
    "temp[\n",
    "    \"result_file_name\"] = f'{temp[\"MAX_BUFFER_SIZE\"]}_{temp[\"TREE_INSERTION_THRESHOLD\"]}_{temp[\"SPLIT_THRESHOLD\"]}_{temp[\"INIT_RESOLUTION\"]}'\n",
    "\n",
    "m,r,k,resolution = temp[\"result_file_name\"].split(\"_\")[:5]\n",
    "with open(f\"tests/results/{temp['result_file_name']}.json\") as f:\n",
    "    data=json.load(f)\n",
    "print(\"insertion time:\",data[\"insertion_time\"])\n",
    "\n",
    "top_res = []\n",
    "query_res=[]\n",
    "for s in [temp]:\n",
    "    tops=[]\n",
    "    query=[]\n",
    "    for fname in iglob(os.path.abspath(f\"tests/results/{s['result_file_name']}*\")):\n",
    "        if \"top\" in fname:\n",
    "            with open(fname) as f:\n",
    "                tops.append(json.load(f))\n",
    "        elif \"query\" in fname:\n",
    "            with open(fname) as f:\n",
    "                query.append(json.load(f))\n",
    "    m,r,k,resolution = s[\"result_file_name\"].split(\"_\")[:5]\n",
    "    tops = pd.DataFrame(tops)\n",
    "    cpu_avg = tops.mean(axis=0)[\"cpu\"]\n",
    "    mem_avg = tops.mean(axis=0)[\"mem\"]\n",
    "    top_res.append({\"R\":r,\"cpu_avg\":cpu_avg,\"mem_avg\":mem_avg})\n",
    "    query = pd.DataFrame(query)\n",
    "    regions_avg = query.mean(axis=0)[\"regions\"]\n",
    "    tid_avg = query.mean(axis=0)[\"tid\"]\n",
    "    query_res.append({\"R\":r,\"query_latency\":regions_avg+tid_avg})\n",
    "pd.DataFrame(top_res).set_index(\"R\")#.to_excel(\"r变量性能监控.xlsx\")\n",
    "# pd.DataFrame(query_res).set_index(\"R\").to_excel(\"r变量查询延迟.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有关MAX_BUFFER_SIZE的插入时间图表\n",
    "# MAX_BUFFER_SIZE是插入阈值：[0, 2, 20, 200, 500, 700]\n",
    "# 横坐标是init_resolution=5\n",
    "# 默认参数，k=2000,r=0.2\n",
    "import json\n",
    "import pandas as  pd\n",
    "settings_default=[]\n",
    "# for m in [0, 2, 20, 200, 500, 700]:\n",
    "for m in [500]:\n",
    "    temp = deepcopy(template)\n",
    "    temp[\"MAX_BUFFER_SIZE\"] = m\n",
    "    temp[\n",
    "        \"result_file_name\"] = f'{temp[\"MAX_BUFFER_SIZE\"]}_{temp[\"TREE_INSERTION_THRESHOLD\"]}_{temp[\"SPLIT_THRESHOLD\"]}_{temp[\"INIT_RESOLUTION\"]}'\n",
    "    settings_default.append(temp)\n",
    "    \n",
    "\n",
    "bucket = []\n",
    "for fname in settings_default:\n",
    "    m,r,k,resolution = fname[\"result_file_name\"].split(\"_\")[:5]\n",
    "    with open(f\"tests/results/{fname['result_file_name']}.json\") as f:\n",
    "        data=json.load(f)\n",
    "        data[\"M\"]=m\n",
    "        data[\"R\"]=r\n",
    "        data[\"K\"]=k\n",
    "        data[\"init_resolution\"]=resolution\n",
    "        bucket.append(data)\n",
    "pd.DataFrame(bucket).set_index(\"M\").to_excel(\"m变量的插入时间-init=1.xlsx\",columns=[\"insertion_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1502/2254837507.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  regions_avg = query.mean(axis=0)[\"regions\"]\n",
      "/tmp/ipykernel_1502/2254837507.py:22: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  tid_avg = query.mean(axis=0)[\"tid\"]\n"
     ]
    }
   ],
   "source": [
    "# 有关MAX_BUFFER_SIZE的性能监控\n",
    "from glob import iglob\n",
    "top_res = []\n",
    "query_res=[]\n",
    "for s in settings_default:\n",
    "    tops=[]\n",
    "    query=[]\n",
    "    for fname in iglob(os.path.abspath(f\"tests/results/{s['result_file_name']}*\")):\n",
    "        if \"top\" in fname:\n",
    "            with open(fname) as f:\n",
    "                tops.append(json.load(f))\n",
    "        elif \"query\" in fname:\n",
    "            with open(fname) as f:\n",
    "                query.append(json.load(f))\n",
    "    m,r,k,resolution = s[\"result_file_name\"].split(\"_\")[:5]\n",
    "    tops = pd.DataFrame(tops)\n",
    "    cpu_avg = tops.mean(axis=0)[\"cpu\"]\n",
    "    mem_avg = tops.mean(axis=0)[\"mem\"]\n",
    "    top_res.append({\"M\":m,\"cpu_avg\":cpu_avg,\"mem_avg\":mem_avg})\n",
    "    query = pd.DataFrame(query)\n",
    "    regions_avg = query.mean(axis=0)[\"regions\"]\n",
    "    tid_avg = query.mean(axis=0)[\"tid\"]\n",
    "    query_res.append({\"M\":m,\"query_latency\":regions_avg+tid_avg})\n",
    "pd.DataFrame(top_res).set_index(\"M\").to_excel(\"m变量性能监控-init=1.xlsx\")\n",
    "pd.DataFrame(query_res).set_index(\"M\").to_excel(\"m变量查询延迟-init=1.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from shapely.geometry import LineString\n",
    "from geopandas import GeoDataFrame\n",
    "trajectory = LineString([(0,0),(1,1),(1,5),(6,7)])\n",
    "GeoDataFrame({\"geometry\":[trajectory,trajectory.buffer(1)]}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our 0.3989702358012437\n",
      "plain 0.23557926720386604\n"
     ]
    }
   ],
   "source": [
    "# 变更查询阈值实验处理\n",
    "import json\n",
    "with open(\"tests/results/our_solution.json\") as f:\n",
    "    data = json.load(f)\n",
    "res = []\n",
    "for k,v in data.items():\n",
    "    res.append(v[\"resp\"][\"regions\"]+v[\"resp\"][\"tid\"])  \n",
    "print(\"our\",sum(res)/len(res),max(res))\n",
    "\n",
    "with open(\"tests/results/plain.json\") as f:\n",
    "    data = json.load(f)\n",
    "res = []\n",
    "for k,v in data.items():\n",
    "    res.append(v[\"resp\"][\"regions\"]+v[\"resp\"][\"tid\"])  \n",
    "print(\"plain\",sum(res)/len(res),max(res))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e82d3c766b925f61f5cab36299b37482cda46d13e70deaf344f7a8eb2295c51"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('beam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
